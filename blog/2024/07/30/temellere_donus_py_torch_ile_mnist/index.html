<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="icon" href="../../../../../img/logo-berat.png" type="image/x-icon" />
    <link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@11.10.0/styles/default.min.css">
    <script src="https://unpkg.com/@highlightjs/cdn-assets@11.10.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <title>Temellere DÃ¶nÃ¼ÅŸ, PyTorch ile MNIST</title>
</head>

<body>
    <div class="max-w-4xl mx-auto px-4">
        <div class="w-full h-14 py-4 flex flex-row justify-between items-center">
            <div>
                <h1 class="text-3xl">Berat Ã‡imen's Blog</h1>
            </div>

            <div>
                <a href="/blog" class="text-xl text-gray-500"> Anasayfa</a>
            </div>
        </div>
    </div>


    <div id="content" class="max-w-4xl mx-auto px-4 py-4">
        <!-- Blog Content -->
        <h1>Temellere DÃ¶nÃ¼ÅŸ, PyTorch ile MNIST</h1>

<p>Selamlar, uzun sÃ¼redir LLM'ler ve Transformer modelleri Ã¼zerine Ã§alÄ±ÅŸÄ±yorum, bu konuda birÃ§ok farklÄ± modeli inceledim ve kendi modellerimi de geliÅŸtirdim. Bu sÃ¼reÃ§te, model geliÅŸtirme sÃ¼recinde en Ã§ok ihtiyaÃ§ duyduÄŸum ÅŸeylerden biri, modelin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± daha iyi anlamak ve modelin iÃ§inde neler olduÄŸunu daha iyi gÃ¶rebilmekti. Daha Ã¶nce temel bir CS (Computer Science) veya ML (Machine Learning) eÄŸitimi almadÄ±m, kariyerimde '<a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Autodidacticism">self-taught</a>' ilerliyorum. SorumluluklarÄ±m genelde <a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>'leri fine-tune etmek ve yeni modeller geliÅŸtirmek Ã¼zerine olduÄŸu iÃ§in temel ML (Machine Learning) konseptlerinden uzaklaÅŸmaya baÅŸladÄ±m. Keras ve Tensorflow ile uzun dÃ¶nem Ã§alÄ±ÅŸma fÄ±rsatÄ± buldum ancak son zamanlarda kendime yaratabildiÄŸim vakitte 'endÃ¼stri standartÄ±' haline gelmiÅŸ <a rel="noopener" target="_blank" href="https://pytorch.org/">PyTorch</a>'a vakit ayÄ±rmak istedim.</p>

<h2>Neler yapacaÄŸÄ±z?</h2>

<p>Bu yazÄ±da, PyTorch ile MNIST veri kÃ¼mesi Ã¼zerinde bir model eÄŸitmek ve bu sÃ¼reÃ§te PyTorch'u daha iyi anlamak istiyorum.</p>

<h2>Peki PyTorch nedir?</h2>

<p>En basit haliyle PyTorch bir "<a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Tensor_(machine_learning)">tensor</a> kÃ¼tÃ¼phanesi". DolayÄ±sÄ±yla temel iÅŸlevi tensorlar (Ã§ok boyutlu array'ler) ile iÅŸlemler yapabilmesi.</p>

<p>Buyurun bakalÄ±m ChatGPT ne demiÅŸ:</p>

<p>PyTorch, aÃ§Ä±k kaynaklÄ± bir makine Ã¶ÄŸrenimi kÃ¼tÃ¼phanesidir ve Ã¶zellikle derin Ã¶ÄŸrenme modellerinin geliÅŸtirilmesi ve eÄŸitilmesi iÃ§in kullanÄ±lÄ±r. 2016 yÄ±lÄ±nda Facebook'un AI Research (FAIR) ekibi tarafÄ±ndan geliÅŸtirilmiÅŸtir. PyTorch, dinamik hesap grafiÄŸi ve sezgisel API tasarÄ±mÄ± ile bilinir, bu da araÅŸtÄ±rmacÄ±lar ve geliÅŸtiriciler tarafÄ±ndan hÄ±zlÄ± bir ÅŸekilde model prototipleri oluÅŸturmayÄ± ve deney yapmayÄ± kolaylaÅŸtÄ±rÄ±r.</p>

<p>BaÅŸlÄ±ca Ã¶zellikleri ÅŸunlardÄ±r:</p>

<p>1 <em>Dinamik Hesap GrafiÄŸi</em>: PyTorch, veri akÄ±ÅŸÄ± sÄ±rasÄ±nda hesap grafiÄŸini dinamik olarak oluÅŸturur. Bu, hata ayÄ±klamayÄ± ve model tasarÄ±mÄ±nÄ± daha kolay ve esnek hale getirir.</p>

<p>2 <em>Otomatik Diferansiyasyon</em>: PyTorch, otomatik tÃ¼rev hesaplama (autograd) Ã¶zelliÄŸi ile gradientleri hesaplamayÄ± ve geri yayÄ±lÄ±mÄ± (backpropagation) otomatikleÅŸtirir.</p>

<p>3 <em>Desteklenen DonanÄ±m</em>: Hem CPU hem de GPU Ã¼zerinde Ã§alÄ±ÅŸabilir, bu da bÃ¼yÃ¼k Ã¶lÃ§ekli hesaplamalarÄ± hÄ±zlandÄ±rÄ±r.</p>

<p>4 <em>GeniÅŸ KapsamlÄ± KullanÄ±m AlanlarÄ±</em>: BilgisayarlÄ± gÃ¶rÃ¼, doÄŸal dil iÅŸleme, sinir aÄŸlarÄ± gibi Ã§eÅŸitli alanlarda kullanÄ±labilir.</p>

<p>5 <em>GeliÅŸmiÅŸ Topluluk ve DokÃ¼mantasyon</em>: PyTorch'un gÃ¼Ã§lÃ¼ bir topluluÄŸu ve geniÅŸ kapsamlÄ± dokÃ¼mantasyonu vardÄ±r, bu da Ã¶ÄŸrenme sÃ¼recini ve problem Ã§Ã¶zmeyi kolaylaÅŸtÄ±rÄ±r.</p>

<p>Bu Ã¶zellikleri sayesinde PyTorch, akademik araÅŸtÄ±rmalarda ve endÃ¼striyel uygulamalarda yaygÄ±n olarak kullanÄ±lan bir kÃ¼tÃ¼phane haline gelmiÅŸtir.</p>

<h2>Ee MNIST ne?</h2>

<p>MNIST, el yazÄ±sÄ± rakamlardan oluÅŸan bir veri kÃ¼mesidir ve genellikle makine Ã¶ÄŸrenimi ve derin Ã¶ÄŸrenme modellerinin eÄŸitiminde kullanÄ±lÄ±r. MNIST veri kÃ¼mesi, 0 ile 9 arasÄ±ndaki rakamlarÄ± temsil eden 28x28 boyutunda siyah-beyaz gÃ¶rÃ¼ntÃ¼lerden oluÅŸur. Her bir gÃ¶rÃ¼ntÃ¼, hangi rakamÄ± temsil ettiÄŸini belirten bir etiketle eÅŸlenir. MNIST veri kÃ¼mesi, genellikle basit bir veri kÃ¼mesi olarak kabul edilir ve yeni bir makine Ã¶ÄŸrenimi modeli oluÅŸtururken ilk adÄ±m olarak sÄ±kÃ§a kullanÄ±lÄ±r.</p>

<p>KÄ±saca ML (Machine Learning) dÃ¼nyasÄ±nda bir 'Hello World' olarak kabul edilir.</p>

<h2>Hadi baÅŸlayalÄ±m</h2>

<h3>Kurulum</h3>

<p>Ã–nce bir virtual environment oluÅŸturup PyTorch kÃ¼tÃ¼phanesini yÃ¼kleyelim:</p>

<p>PyTorch dÃ¶kÃ¼mantasyonuna <a rel="noopener" target="_blank" href="https://pytorch.org/get-started/locally/">buradan</a> ulaÅŸabilirsiniz.</p>

<pre><code>python -m venv .venv
source .venv/bin/activate
pip install torch torchvision torchaudio
</code></pre>

<p>(Opsiyonel) Ek olarak Keras'tan alÄ±ÅŸkÄ±n olduÄŸum model Ã¶zetlerini gÃ¶rebilmek iÃ§in torchinfo kÃ¼tÃ¼phanesini de yÃ¼kleyelim:</p>

<pre><code>pip install torchinfo
</code></pre>

<p>Bu sÃ¼reÃ§te VSCode kullanacaÄŸÄ±m ve Jupyter Notebook'u VSCode Ã¼zerinde Ã§alÄ±ÅŸtÄ±racaÄŸÄ±m.</p>

<h3>Hadi kÃ¼tÃ¼phaneleri import edelim</h3>

<pre><code>import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary
</code></pre>

<h3>TransformlarÄ± tanÄ±mlayalÄ±m</h3>

<pre><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
</code></pre>

<p>Transformlar nedir? Transformlar, veri kÃ¼mesindeki gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde Ã§eÅŸitli iÅŸlemler yapmamÄ±zÄ± saÄŸlar. Ã–rneÄŸin, ToTensor() metodu, gÃ¶rÃ¼ntÃ¼yÃ¼ tensora dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Normalize() metodu, gÃ¶rÃ¼ntÃ¼ piksellerini belirli bir ortalama ve standart sapma ile normalize eder. Bu iÅŸlemler, veri kÃ¼mesinin daha iyi eÄŸitilmesine ve modelin daha iyi performans gÃ¶stermesine yardÄ±mcÄ± olur Ã§oÄŸunlukla.</p>

<p>KÄ±saca: Verimizi standart bir hale getirmeye Ã§alÄ±ÅŸÄ±yoruz.</p>

<h3>Veri kÃ¼mesini yÃ¼kleyelim</h3>

<pre><code># Train
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# Test
testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
</code></pre>

<p><strong>trainset</strong> ile eÄŸitim veri kÃ¼mesini, <strong>testset</strong> ile test veri kÃ¼mesini yÃ¼klÃ¼yoruz. <strong>trainloader</strong> ve <strong>testloader</strong> ile de bu veri kÃ¼melerini daha rahat kullanabilmek iÃ§in DataLoader'a yÃ¼klÃ¼yoruz. Ã‡ok dÃ¼zgÃ¼n bir aÃ§Ä±klama olmadÄ± bu ama anladÄ±ÄŸÄ±m kadarÄ±yla PyTorch'da veriler <strong>DataLoader</strong>'lar ile yÃ¼kleniyor ve bu DataLoader'lar Ã¼zerinden veri kÃ¼mesine eriÅŸim saÄŸlanÄ±yor.</p>

<p><strong>batch_size</strong> parametresi, her bir adÄ±mda kaÃ§ verinin iÅŸleneceÄŸini belirler. <strong>shuffle</strong> parametresi ise veri kÃ¼mesinin her bir epoch'ta karÄ±ÅŸtÄ±rÄ±lÄ±p karÄ±ÅŸtÄ±rÄ±lmayacaÄŸÄ±nÄ± belirler.</p>

<h3>Modeli tanÄ±mlayalÄ±m</h3>

<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
</code></pre>

<p>AdÄ±m adÄ±m gidelim:</p>

<ol>
<li><strong>Net</strong> adÄ±nda bir sÄ±nÄ±f tanÄ±mlÄ±yoruz ve bu sÄ±nÄ±f <strong>nn.Module</strong>'den tÃ¼retiliyor.</li>
<li><strong>__init__</strong> metodu, sÄ±nÄ±fÄ±n baÅŸlatÄ±cÄ± metodu olup, modelin katmanlarÄ±nÄ± tanÄ±mlar.</li>
<li><strong>forward</strong> metodu, modelin ileri geÃ§iÅŸini tanÄ±mlar. Ä°leri geÃ§iÅŸ, modelin girdisini alÄ±r ve Ã§Ä±ktÄ±yÄ± Ã¼retir.</li>
<li><strong>nn.Linear</strong> metodu, tam baÄŸlÄ± bir katman oluÅŸturur. Ä°</li>
<li><strong>F.relu</strong> metodu, ReLU aktivasyon fonksiyonunu uygular.</li>
<li><strong>x.view(-1, 28 * 28)</strong>, girdiyi yeniden ÅŸekillendirir. -1, girdinin boyutunu otomatik olarak ayarlar.</li>
<li><strong>x = F.relu(self.fc1(x))</strong>, girdiyi ilk tam baÄŸlÄ± katmandan geÃ§irir ve ReLU aktivasyon fonksiyonunu uygular.</li>
<li><strong>x = F.relu(self.fc2(x))</strong>, girdiyi ikinci tam baÄŸlÄ± katmandan geÃ§irir ve ReLU aktivasyon fonksiyonunu uygular.</li>
<li><strong>x = self.fc3(x)</strong>, girdiyi Ã¼Ã§Ã¼ncÃ¼ tam baÄŸlÄ± katmandan geÃ§irir ve Ã§Ä±ktÄ±yÄ± Ã¼retir.</li>
<li><strong>return x</strong>, Ã§Ä±ktÄ±yÄ± dÃ¶ndÃ¼rÃ¼r.</li>
</ol>

<p>Burada model parametrelerini kafama gÃ¶re belirledim, daha iyi bir model iÃ§in parametre optimizasyonu yapÄ±labilir. Bunlar biraz daha deneme yanÄ±lma ile Ã¶ÄŸrenilecek ÅŸeyler bana kalÄ±rsa.</p>

<h3>Modeli, Loss fonksiyonunu ve Optimizer'Ä± tanÄ±mlayalÄ±m</h3>

<pre><code>net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
</code></pre>

<ol>
<li><strong>net = Net()</strong>, modeli oluÅŸturur.</li>
<li><strong>criterion = nn.CrossEntropyLoss()</strong>, loss fonksiyonunu tanÄ±mlar. CrossEntropyLoss, sÄ±nÄ±flandÄ±rma problemleri iÃ§in yaygÄ±n olarak kullanÄ±lan bir loss fonksiyonudur.</li>
<li><strong>optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</strong>, optimizer'Ä± tanÄ±mlar. Adam optimizer, gradient tabanlÄ± optimizasyon algoritmalarÄ±ndan biridir.</li>
</ol>

<p>Burada da Loss fonksiyonunu ve Optimizer'Ä± tecrÃ¼beme dayanarak belirledim. Daha iyi bir model iÃ§in bu parametrelerin deÄŸiÅŸtirilmesi gerekebilir. Ã–rneÄŸin Adam optimizer yerine SGD optimizer kullanÄ±labilir.</p>

<h3>Model Ã¶zetini gÃ¶relim</h3>

<pre><code>summary(net, input_size=(64, 1, 28, 28))
</code></pre>

<p>Model Ã¶zetini gÃ¶rmek iÃ§in <strong>torchinfo</strong> kÃ¼tÃ¼phanesini kullandÄ±m. Bu kÃ¼tÃ¼phane, modelin katmanlarÄ±nÄ± ve parametrelerini gÃ¶sterir. Burada <strong>inputsize</strong> parametresi, modelin girdi boyutunu belirtir. (batch_size, channels, height, width)</p>

<p>Model Ã¶zeti aÅŸaÄŸÄ±daki gibi olacak:</p>

<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Net                                      [64, 10]                  --
â”œâ”€Linear: 1-1                            [64, 512]                 401,920
â”œâ”€Linear: 1-2                            [64, 256]                 131,328
â”œâ”€Linear: 1-3                            [64, 10]                  2,570
==========================================================================================
Total params: 535,818
Trainable params: 535,818
Non-trainable params: 0
Total mult-adds (M): 34.29
==========================================================================================
Input size (MB): 0.20
Forward/backward pass size (MB): 0.40
Params size (MB): 2.14
Estimated Total Size (MB): 2.74
==========================================================================================
</code></pre>

<h3>Modeli eÄŸitelim</h3>

<pre><code>for epoch in range(5):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:    # print every 100 mini-batches
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')
</code></pre>

<ol>
<li><strong>for epoch in range(5)</strong>, 5 epoch boyunca eÄŸitim veri kÃ¼mesi Ã¼zerinde dÃ¶ngÃ¼ oluÅŸturur.</li>
<li><strong>running_loss = 0.0</strong>, loss deÄŸerini sÄ±fÄ±rlar.</li>
<li><strong>for i, data in enumerate(trainloader, 0)</strong>, eÄŸitim veri kÃ¼mesinde dÃ¶ngÃ¼ oluÅŸturur.</li>
<li><strong>inputs, labels = data</strong>, veri kÃ¼mesinden girdi ve etiketleri alÄ±r.</li>
<li><strong>optimizer.zero_grad()</strong>, gradyanlarÄ± sÄ±fÄ±rlar.</li>
<li><strong>outputs = net(inputs)</strong>, modeli eÄŸitir.</li>
<li><strong>loss = criterion(outputs, labels)</strong>, loss deÄŸerini hesaplar.</li>
<li><strong>loss.backward()</strong>, gradyanlarÄ± hesaplar.</li>
<li><strong>optimizer.step()</strong>, modeli gÃ¼nceller.</li>
<li><strong>running_loss += loss.item()</strong>, loss deÄŸerini toplar.</li>
<li><strong>if i % 100 == 99</strong>, her 100 mini-batch'te loss deÄŸerini yazdÄ±rÄ±r.</li>
<li><strong>print('Finished Training')</strong>, eÄŸitimi tamamlar.</li>
</ol>

<p>Burada Keras'a gÃ¶re biraz daha fazla kod yazdÄ±m. PyTorch'da <em>training loop</em> yazarken daha fazla kontrol sahibi oluyorsunuz. Bu, modelin nasÄ±l eÄŸitildiÄŸini daha iyi anlamanÄ±za yardÄ±mcÄ± olabilir. Biraz daha proje yapmaya ihtiyacÄ±m var.</p>

<h3>Modeli test edelim</h3>

<pre><code>correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
</code></pre>

<ol>
<li><strong>correct = 0, total = 0</strong>, doÄŸru tahmin sayÄ±sÄ±nÄ± ve toplam tahmin sayÄ±sÄ±nÄ± sÄ±fÄ±rlar.</li>
<li><strong>with torch.no_grad()</strong>, gradyanlarÄ± kapatÄ±r. Bu, modelin eÄŸitim sÄ±rasÄ±nda gradyanlarÄ± gÃ¼ncellememesini saÄŸlar.</li>
<li><strong>for data in testloader</strong>, test veri kÃ¼mesinde dÃ¶ngÃ¼ oluÅŸturur.</li>
<li><strong>images, labels = data</strong>, veri kÃ¼mesinden girdi ve etiketleri alÄ±r.</li>
<li><strong>outputs = net(images)</strong>, modeli test eder.</li>
<li><strong>_, predicted = torch.max(outputs.data, 1)</strong>, tahminleri alÄ±r.</li>
<li><strong>total += labels.size(0)</strong>, toplam tahmin sayÄ±sÄ±nÄ± artÄ±rÄ±r.</li>
<li><strong>correct += (predicted == labels).sum().item()</strong>, doÄŸru tahmin sayÄ±sÄ±nÄ± artÄ±rÄ±r.</li>
<li><strong>print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')</strong>, doÄŸruluk oranÄ±nÄ± yazdÄ±rÄ±r.</li>
</ol>

<h2>SonuÃ§</h2>

<p>ML tarafÄ±nda yapabileceÄŸimiz en basit projelerden biri buydu, PyTorch'u Ã§ok daha iyi anladÄ±ÄŸÄ±mÄ± dÃ¼ÅŸÃ¼nÃ¼yorum ancak yazdÄ±ÄŸÄ±m kodun daha dÃ¼zgÃ¼n olmasÄ± adÄ±na ChatGPT ve Stackoverflow gibi kaynaklardan destek aldÄ±m, ne yazÄ±k ki geÃ§miÅŸ tecrÃ¼bemi gÃ¶z Ã¶nÃ¼nde bulundurunca benim iÃ§in Ã¶ÄŸrenme konusunda bir tÄ±k olumsuz bir tecrÃ¼be oldu. Ancak yine de PyTorch'u daha iyi anlamak iÃ§in bu tarz basit projelerin yapÄ±lmasÄ± gerektiÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum. UmarÄ±m bu yazÄ±, PyTorch'u daha iyi anlamak isteyenlere yardÄ±mcÄ± olur.</p>

<p>AyrÄ±ca bu blok yazÄ±sÄ±nÄ± yazabilmek iÃ§in web siteme sÄ±fÄ±rdan blog mekanizmasÄ± eklemem gerekti ğŸ˜…, bunu da dÃ¶kÃ¼mante etmek istiyorum.</p>

<p>GÃ¶rÃ¼ÅŸmek Ã¼zere!</p>

    </div>

    <script>
        document.querySelectorAll('#content h1').forEach((item) => {
            item.classList.add('text-2xl', 'font-semibold', 'mt-4', 'mb-4', 'text-gray-950');
        });

        document.querySelectorAll('#content h2').forEach((item) => {
            item.classList.add('text-xl', 'font-semibold', 'mt-4', 'mb-4', 'text-gray-900');
        });

        document.querySelectorAll('#content h3').forEach((item) => {
            item.classList.add('text-lg', 'font-semibold', 'mt-4', 'mb-4', 'text-gray-800');
        });

        document.querySelectorAll('#content p').forEach((item) => {
            item.classList.add('text-base', 'text-gray-700', 'mt-2', 'mb-2');
        });

        document.querySelectorAll('#content a').forEach((item) => {
            item.classList.add('text-base', 'text-blue-500');
        });

        document.querySelectorAll('#content em').forEach((item) => {
            item.classList.add('text-base', 'text-gray-900');
        });

        document.querySelectorAll('#content strong').forEach((item) => {
            item.classList.add('text-base', 'text-gray-900', 'font-semibold');
        });

        document.querySelectorAll('#content code').forEach((item) => {
            item.classList.add('p-1', 'rounded', "mt-2", "mb-2");
        });


        document.querySelectorAll('#content ol').forEach((ol) => {
            ol.querySelectorAll('li').forEach((item, index) => {
                item.innerHTML = `${index + 1}. ${item.innerHTML}`;
            });
        });

        document.querySelectorAll('#content ul').forEach((ul) => {
            ul.classList.add('list-disc', 'list-inside');
        });

    </script>
</body>

</html>