# Temellere DÃ¶nÃ¼ÅŸ, PyTorch ile MNIST

Selamlar, uzun sÃ¼redir LLM'ler ve Transformer modelleri Ã¼zerine Ã§alÄ±ÅŸÄ±yorum, bu konuda birÃ§ok farklÄ± modeli inceledim ve kendi modellerimi de geliÅŸtirdim. Bu sÃ¼reÃ§te, model geliÅŸtirme sÃ¼recinde en Ã§ok ihtiyaÃ§ duyduÄŸum ÅŸeylerden biri, modelin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± daha iyi anlamak ve modelin iÃ§inde neler olduÄŸunu daha iyi gÃ¶rebilmekti. Daha Ã¶nce temel bir CS (Computer Science) veya ML (Machine Learning) eÄŸitimi almadÄ±m, kariyerimde '[self-taught](https://en.wikipedia.org/wiki/Autodidacticism)' ilerliyorum. SorumluluklarÄ±m genelde [LLM](https://en.wikipedia.org/wiki/Large_language_model)'leri fine-tune etmek ve yeni modeller geliÅŸtirmek Ã¼zerine olduÄŸu iÃ§in temel ML (Machine Learning) konseptlerinden uzaklaÅŸmaya baÅŸladÄ±m. Keras ve Tensorflow ile uzun dÃ¶nem Ã§alÄ±ÅŸma fÄ±rsatÄ± buldum ancak son zamanlarda kendime yaratabildiÄŸim vakitte 'endÃ¼stri standartÄ±' haline gelmiÅŸ [PyTorch](https://pytorch.org/)'a vakit ayÄ±rmak istedim.

## Neler yapacaÄŸÄ±z?

Bu yazÄ±da, PyTorch ile MNIST veri kÃ¼mesi Ã¼zerinde bir model eÄŸitmek ve bu sÃ¼reÃ§te PyTorch'u daha iyi anlamak istiyorum.

## Peki PyTorch nedir?

En basit haliyle PyTorch bir "[tensor](<https://en.wikipedia.org/wiki/Tensor_(machine_learning)>) kÃ¼tÃ¼phanesi". DolayÄ±sÄ±yla temel iÅŸlevi tensorlar (Ã§ok boyutlu array'ler) ile iÅŸlemler yapabilmesi.

Buyurun bakalÄ±m ChatGPT ne demiÅŸ:

PyTorch, aÃ§Ä±k kaynaklÄ± bir makine Ã¶ÄŸrenimi kÃ¼tÃ¼phanesidir ve Ã¶zellikle derin Ã¶ÄŸrenme modellerinin geliÅŸtirilmesi ve eÄŸitilmesi iÃ§in kullanÄ±lÄ±r. 2016 yÄ±lÄ±nda Facebook'un AI Research (FAIR) ekibi tarafÄ±ndan geliÅŸtirilmiÅŸtir. PyTorch, dinamik hesap grafiÄŸi ve sezgisel API tasarÄ±mÄ± ile bilinir, bu da araÅŸtÄ±rmacÄ±lar ve geliÅŸtiriciler tarafÄ±ndan hÄ±zlÄ± bir ÅŸekilde model prototipleri oluÅŸturmayÄ± ve deney yapmayÄ± kolaylaÅŸtÄ±rÄ±r.

BaÅŸlÄ±ca Ã¶zellikleri ÅŸunlardÄ±r:

1 _Dinamik Hesap GrafiÄŸi_: PyTorch, veri akÄ±ÅŸÄ± sÄ±rasÄ±nda hesap grafiÄŸini dinamik olarak oluÅŸturur. Bu, hata ayÄ±klamayÄ± ve model tasarÄ±mÄ±nÄ± daha kolay ve esnek hale getirir.

2 _Otomatik Diferansiyasyon_: PyTorch, otomatik tÃ¼rev hesaplama (autograd) Ã¶zelliÄŸi ile gradientleri hesaplamayÄ± ve geri yayÄ±lÄ±mÄ± (backpropagation) otomatikleÅŸtirir.

3 _Desteklenen DonanÄ±m_: Hem CPU hem de GPU Ã¼zerinde Ã§alÄ±ÅŸabilir, bu da bÃ¼yÃ¼k Ã¶lÃ§ekli hesaplamalarÄ± hÄ±zlandÄ±rÄ±r.

4 _GeniÅŸ KapsamlÄ± KullanÄ±m AlanlarÄ±_: BilgisayarlÄ± gÃ¶rÃ¼, doÄŸal dil iÅŸleme, sinir aÄŸlarÄ± gibi Ã§eÅŸitli alanlarda kullanÄ±labilir.

5 _GeliÅŸmiÅŸ Topluluk ve DokÃ¼mantasyon_: PyTorch'un gÃ¼Ã§lÃ¼ bir topluluÄŸu ve geniÅŸ kapsamlÄ± dokÃ¼mantasyonu vardÄ±r, bu da Ã¶ÄŸrenme sÃ¼recini ve problem Ã§Ã¶zmeyi kolaylaÅŸtÄ±rÄ±r.

Bu Ã¶zellikleri sayesinde PyTorch, akademik araÅŸtÄ±rmalarda ve endÃ¼striyel uygulamalarda yaygÄ±n olarak kullanÄ±lan bir kÃ¼tÃ¼phane haline gelmiÅŸtir.

## Ee MNIST ne?

MNIST, el yazÄ±sÄ± rakamlardan oluÅŸan bir veri kÃ¼mesidir ve genellikle makine Ã¶ÄŸrenimi ve derin Ã¶ÄŸrenme modellerinin eÄŸitiminde kullanÄ±lÄ±r. MNIST veri kÃ¼mesi, 0 ile 9 arasÄ±ndaki rakamlarÄ± temsil eden 28x28 boyutunda siyah-beyaz gÃ¶rÃ¼ntÃ¼lerden oluÅŸur. Her bir gÃ¶rÃ¼ntÃ¼, hangi rakamÄ± temsil ettiÄŸini belirten bir etiketle eÅŸlenir. MNIST veri kÃ¼mesi, genellikle basit bir veri kÃ¼mesi olarak kabul edilir ve yeni bir makine Ã¶ÄŸrenimi modeli oluÅŸtururken ilk adÄ±m olarak sÄ±kÃ§a kullanÄ±lÄ±r.

KÄ±saca ML (Machine Learning) dÃ¼nyasÄ±nda bir 'Hello World' olarak kabul edilir.

## Hadi baÅŸlayalÄ±m

### Kurulum

Ã–nce bir virtual environment oluÅŸturup PyTorch kÃ¼tÃ¼phanesini yÃ¼kleyelim:

PyTorch dÃ¶kÃ¼mantasyonuna [buradan](https://pytorch.org/get-started/locally/) ulaÅŸabilirsiniz.

```bash
python -m venv .venv
source .venv/bin/activate
pip install torch torchvision torchaudio
```

(Opsiyonel) Ek olarak Keras'tan alÄ±ÅŸkÄ±n olduÄŸum model Ã¶zetlerini gÃ¶rebilmek iÃ§in torchinfo kÃ¼tÃ¼phanesini de yÃ¼kleyelim:

```bash
pip install torchinfo
```

Bu sÃ¼reÃ§te VSCode kullanacaÄŸÄ±m ve Jupyter Notebook'u VSCode Ã¼zerinde Ã§alÄ±ÅŸtÄ±racaÄŸÄ±m.

### Hadi kÃ¼tÃ¼phaneleri import edelim

```python
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary
```

### TransformlarÄ± tanÄ±mlayalÄ±m

```python
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
```

Transformlar nedir? Transformlar, veri kÃ¼mesindeki gÃ¶rÃ¼ntÃ¼ler Ã¼zerinde Ã§eÅŸitli iÅŸlemler yapmamÄ±zÄ± saÄŸlar. Ã–rneÄŸin, ToTensor() metodu, gÃ¶rÃ¼ntÃ¼yÃ¼ tensora dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Normalize() metodu, gÃ¶rÃ¼ntÃ¼ piksellerini belirli bir ortalama ve standart sapma ile normalize eder. Bu iÅŸlemler, veri kÃ¼mesinin daha iyi eÄŸitilmesine ve modelin daha iyi performans gÃ¶stermesine yardÄ±mcÄ± olur Ã§oÄŸunlukla.

KÄ±saca: Verimizi standart bir hale getirmeye Ã§alÄ±ÅŸÄ±yoruz.

### Veri kÃ¼mesini yÃ¼kleyelim

```python
# Train
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# Test
testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
```

**trainset** ile eÄŸitim veri kÃ¼mesini, **testset** ile test veri kÃ¼mesini yÃ¼klÃ¼yoruz. **trainloader** ve **testloader** ile de bu veri kÃ¼melerini daha rahat kullanabilmek iÃ§in DataLoader'a yÃ¼klÃ¼yoruz. Ã‡ok dÃ¼zgÃ¼n bir aÃ§Ä±klama olmadÄ± bu ama anladÄ±ÄŸÄ±m kadarÄ±yla PyTorch'da veriler **DataLoader**'lar ile yÃ¼kleniyor ve bu DataLoader'lar Ã¼zerinden veri kÃ¼mesine eriÅŸim saÄŸlanÄ±yor.

**batch_size** parametresi, her bir adÄ±mda kaÃ§ verinin iÅŸleneceÄŸini belirler. **shuffle** parametresi ise veri kÃ¼mesinin her bir epoch'ta karÄ±ÅŸtÄ±rÄ±lÄ±p karÄ±ÅŸtÄ±rÄ±lmayacaÄŸÄ±nÄ± belirler.

### Modeli tanÄ±mlayalÄ±m

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
```

AdÄ±m adÄ±m gidelim:

1. **Net** adÄ±nda bir sÄ±nÄ±f tanÄ±mlÄ±yoruz ve bu sÄ±nÄ±f **nn.Module**'den tÃ¼retiliyor.
2. **\_\_init\_\_** metodu, sÄ±nÄ±fÄ±n baÅŸlatÄ±cÄ± metodu olup, modelin katmanlarÄ±nÄ± tanÄ±mlar.
3. **forward** metodu, modelin ileri geÃ§iÅŸini tanÄ±mlar. Ä°leri geÃ§iÅŸ, modelin girdisini alÄ±r ve Ã§Ä±ktÄ±yÄ± Ã¼retir.
4. **nn.Linear** metodu, tam baÄŸlÄ± bir katman oluÅŸturur. Ä°
5. **F.relu** metodu, ReLU aktivasyon fonksiyonunu uygular.
6. **x.view(-1, 28 \* 28)**, girdiyi yeniden ÅŸekillendirir. -1, girdinin boyutunu otomatik olarak ayarlar.
7. **x = F.relu(self.fc1(x))**, girdiyi ilk tam baÄŸlÄ± katmandan geÃ§irir ve ReLU aktivasyon fonksiyonunu uygular.
8. **x = F.relu(self.fc2(x))**, girdiyi ikinci tam baÄŸlÄ± katmandan geÃ§irir ve ReLU aktivasyon fonksiyonunu uygular.
9. **x = self.fc3(x)**, girdiyi Ã¼Ã§Ã¼ncÃ¼ tam baÄŸlÄ± katmandan geÃ§irir ve Ã§Ä±ktÄ±yÄ± Ã¼retir.
10. **return x**, Ã§Ä±ktÄ±yÄ± dÃ¶ndÃ¼rÃ¼r.

Burada model parametrelerini kafama gÃ¶re belirledim, daha iyi bir model iÃ§in parametre optimizasyonu yapÄ±labilir. Bunlar biraz daha deneme yanÄ±lma ile Ã¶ÄŸrenilecek ÅŸeyler bana kalÄ±rsa.

### Modeli, Loss fonksiyonunu ve Optimizer'Ä± tanÄ±mlayalÄ±m

```python
net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
```

1. **net = Net()**, modeli oluÅŸturur.
2. **criterion = nn.CrossEntropyLoss()**, loss fonksiyonunu tanÄ±mlar. CrossEntropyLoss, sÄ±nÄ±flandÄ±rma problemleri iÃ§in yaygÄ±n olarak kullanÄ±lan bir loss fonksiyonudur.
3. **optimizer = torch.optim.Adam(net.parameters(), lr=0.001)**, optimizer'Ä± tanÄ±mlar. Adam optimizer, gradient tabanlÄ± optimizasyon algoritmalarÄ±ndan biridir.

Burada da Loss fonksiyonunu ve Optimizer'Ä± tecrÃ¼beme dayanarak belirledim. Daha iyi bir model iÃ§in bu parametrelerin deÄŸiÅŸtirilmesi gerekebilir. Ã–rneÄŸin Adam optimizer yerine SGD optimizer kullanÄ±labilir.

### Model Ã¶zetini gÃ¶relim

```python
summary(net, input_size=(64, 1, 28, 28))
```

Model Ã¶zetini gÃ¶rmek iÃ§in **torchinfo** kÃ¼tÃ¼phanesini kullandÄ±m. Bu kÃ¼tÃ¼phane, modelin katmanlarÄ±nÄ± ve parametrelerini gÃ¶sterir. Burada **inputsize** parametresi, modelin girdi boyutunu belirtir. (batch_size, channels, height, width)

Model Ã¶zeti aÅŸaÄŸÄ±daki gibi olacak:

```bash
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Net                                      [64, 10]                  --
â”œâ”€Linear: 1-1                            [64, 512]                 401,920
â”œâ”€Linear: 1-2                            [64, 256]                 131,328
â”œâ”€Linear: 1-3                            [64, 10]                  2,570
==========================================================================================
Total params: 535,818
Trainable params: 535,818
Non-trainable params: 0
Total mult-adds (M): 34.29
==========================================================================================
Input size (MB): 0.20
Forward/backward pass size (MB): 0.40
Params size (MB): 2.14
Estimated Total Size (MB): 2.74
==========================================================================================
```

### Modeli eÄŸitelim

```python
for epoch in range(5):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if i % 100 == 99:    # print every 100 mini-batches
            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0

print('Finished Training')
```

1. **for epoch in range(5)**, 5 epoch boyunca eÄŸitim veri kÃ¼mesi Ã¼zerinde dÃ¶ngÃ¼ oluÅŸturur.
2. **running_loss = 0.0**, loss deÄŸerini sÄ±fÄ±rlar.
3. **for i, data in enumerate(trainloader, 0)**, eÄŸitim veri kÃ¼mesinde dÃ¶ngÃ¼ oluÅŸturur.
4. **inputs, labels = data**, veri kÃ¼mesinden girdi ve etiketleri alÄ±r.
5. **optimizer.zero_grad()**, gradyanlarÄ± sÄ±fÄ±rlar.
6. **outputs = net(inputs)**, modeli eÄŸitir.
7. **loss = criterion(outputs, labels)**, loss deÄŸerini hesaplar.
8. **loss.backward()**, gradyanlarÄ± hesaplar.
9. **optimizer.step()**, modeli gÃ¼nceller.
10. **running_loss += loss.item()**, loss deÄŸerini toplar.
11. **if i % 100 == 99**, her 100 mini-batch'te loss deÄŸerini yazdÄ±rÄ±r.
12. **print('Finished Training')**, eÄŸitimi tamamlar.

Burada Keras'a gÃ¶re biraz daha fazla kod yazdÄ±m. PyTorch'da _training loop_ yazarken daha fazla kontrol sahibi oluyorsunuz. Bu, modelin nasÄ±l eÄŸitildiÄŸini daha iyi anlamanÄ±za yardÄ±mcÄ± olabilir. Biraz daha proje yapmaya ihtiyacÄ±m var.

### Modeli test edelim

```python
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
```

1. **correct = 0, total = 0**, doÄŸru tahmin sayÄ±sÄ±nÄ± ve toplam tahmin sayÄ±sÄ±nÄ± sÄ±fÄ±rlar.
2. **with torch.no_grad()**, gradyanlarÄ± kapatÄ±r. Bu, modelin eÄŸitim sÄ±rasÄ±nda gradyanlarÄ± gÃ¼ncellememesini saÄŸlar.
3. **for data in testloader**, test veri kÃ¼mesinde dÃ¶ngÃ¼ oluÅŸturur.
4. **images, labels = data**, veri kÃ¼mesinden girdi ve etiketleri alÄ±r.
5. **outputs = net(images)**, modeli test eder.
6. **\_, predicted = torch.max(outputs.data, 1)**, tahminleri alÄ±r.
7. **total += labels.size(0)**, toplam tahmin sayÄ±sÄ±nÄ± artÄ±rÄ±r.
8. **correct += (predicted == labels).sum().item()**, doÄŸru tahmin sayÄ±sÄ±nÄ± artÄ±rÄ±r.
9. **print(f'Accuracy of the network on the 10000 test images: {100 \* correct / total:.2f}%')**, doÄŸruluk oranÄ±nÄ± yazdÄ±rÄ±r.

## SonuÃ§

ML tarafÄ±nda yapabileceÄŸimiz en basit projelerden biri buydu, PyTorch'u Ã§ok daha iyi anladÄ±ÄŸÄ±mÄ± dÃ¼ÅŸÃ¼nÃ¼yorum ancak yazdÄ±ÄŸÄ±m kodun daha dÃ¼zgÃ¼n olmasÄ± adÄ±na ChatGPT ve Stackoverflow gibi kaynaklardan destek aldÄ±m, ne yazÄ±k ki geÃ§miÅŸ tecrÃ¼bemi gÃ¶z Ã¶nÃ¼nde bulundurunca benim iÃ§in Ã¶ÄŸrenme konusunda bir tÄ±k olumsuz bir tecrÃ¼be oldu. Ancak yine de PyTorch'u daha iyi anlamak iÃ§in bu tarz basit projelerin yapÄ±lmasÄ± gerektiÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum. UmarÄ±m bu yazÄ±, PyTorch'u daha iyi anlamak isteyenlere yardÄ±mcÄ± olur.

Projenin olduÄŸu Github reposuna [buradan](https://github.com/beratcmn/pytorch-mnist) ulaÅŸabilirsiniz.

AyrÄ±ca bu blok yazÄ±sÄ±nÄ± yazabilmek iÃ§in web siteme sÄ±fÄ±rdan blog mekanizmasÄ± eklemem gerekti ğŸ˜…, bunu da dÃ¶kÃ¼mante etmek istiyorum.

GÃ¶rÃ¼ÅŸmek Ã¼zere!